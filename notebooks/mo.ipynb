{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b0a247",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (/Users/musharafmaqbool/opt/anaconda3/envs/env1/lib/python3.10/site-packages/keras/preprocessing/sequence.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Constant\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvolutional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2D\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (/Users/musharafmaqbool/opt/anaconda3/envs/env1/lib/python3.10/site-packages/keras/preprocessing/sequence.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import ReduceLROnPlateau,CSVLogger\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,LSTM,BatchNormalization,Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Embedding,Conv1D, GlobalMaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,classification_report\n",
    "import shutil\n",
    "\n",
    "\n",
    "#---------Data Read & Transformation---------\n",
    "\n",
    "#Data Reading for  Dataset\n",
    "nRowsRead = None # specify 'None' to read complete file\n",
    "df0 = pd.read_csv('data file here', delimiter=',', nrows = nRowsRead)\n",
    "nRow, nCol = df0.shape\n",
    "print('There are {} rows and {} columns'.format(nRow, nCol))\n",
    "\n",
    "#Doing Transformation\n",
    "c=df0['class']\n",
    "df0.rename(columns={'tweet' : 'text',\n",
    "                   'class' : 'category'}, \n",
    "                    inplace=True)\n",
    "a=df0['text']\n",
    "b=df0['category'].map({0: 'hate_speech', 1: 'offensive_language',2: 'neither'})\n",
    "df= pd.concat([a,b,c], axis=1)\n",
    "df.rename(columns={'class' : 'label'}, \n",
    "                    inplace=True)\n",
    "\n",
    "hate, ofensive, neither = np.bincount(df['label'])\n",
    "total = hate + ofensive + neither\n",
    "print('Examples:\\n    Total: {}\\n    hate: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, hate, 100 * hate / total))\n",
    "print('Examples:\\n    Total: {}\\n    Ofensive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, ofensive, 100 * ofensive / total))\n",
    "print('Examples:\\n    Total: {}\\n    Neither: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, neither, 100 * neither / total))\n",
    "    \n",
    "x= df['text']\n",
    "y=df['label']\n",
    "\n",
    "texts = x\n",
    "target = y\n",
    "#tokensing the data\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(texts)\n",
    "#defining vocabulary length\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "def embed(text_data): \n",
    "    return word_tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "longest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_train))\n",
    "\n",
    "#padding_data\n",
    "train_padded_sentences = pad_sequences(\n",
    "    embed(texts), \n",
    "    length_long_sentence, \n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "embedding_dim = 100\n",
    "\n",
    "# Loading GloVe-100D embedding_file\n",
    "with open('/kaggle/input/glove6b100dtxt/glove.6B.100d.txt') as fp:\n",
    "    for line in fp.readlines():\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "# Loading embedding_vectors of words which comes in Glove files other will be equated to 0\n",
    "#defining embedding matrix shape\n",
    "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "#creating embedding matrix\n",
    "for word, index in word_tokenizer.word_index.items(): \n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        \n",
    "#splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_padded_sentences, \n",
    "    target, \n",
    "    test_size=0.25\n",
    ")\n",
    "\n",
    "X_train, x_val, y_train, y_val = train_test_split(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    test_size=0.1 )\n",
    "\n",
    "\n",
    "#defining glove bilstm model\n",
    "def bilstm():\n",
    "    model = Sequential()\n",
    "    #adding embediing layer\n",
    "    model.add(Embedding(\n",
    "        input_dim=embedding_matrix.shape[0], \n",
    "        output_dim=embedding_matrix.shape[1], \n",
    "        weights = [embedding_matrix], \n",
    "        input_length=length_long_sentence))\n",
    "    #adding Bi_lstm later\n",
    "    model.add(Bidirectional(LSTM(\n",
    "        length_long_sentence, \n",
    "        return_sequences = True, \n",
    "        recurrent_dropout=0.2)))\n",
    "    model.add(GlobalMaxPool1D()) #globalmaxpooling_layer\n",
    "    model.add(BatchNormalization()) #bath_normalisation\n",
    "    model.add(Dropout(0.5)) #dropout_1\n",
    "    model.add(Dense(length_long_sentence, activation = \"relu\")) #denselayer_1\n",
    "    model.add(Dropout(0.5)) #dropout_2\n",
    "    model.add(Dense(length_long_sentence, activation = \"relu\")) #denselayer_2\n",
    "    model.add(Dropout(0.5)) #dropout_3\n",
    "    model.add(Dense(3, activation = 'softmax')) #classification_layer\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "bilstm_model = bilstm()\n",
    "\n",
    "#defining_class_weight for each class\n",
    "weight_class1 = (1 / hate)*(total)/3.0 \n",
    "weight_class2 = (1 / ofensive)*(total)/3.0\n",
    "weight_class3 = (1 / neither)*(total)/3.0\n",
    "class_weight = {0: weight_class1, 1: weight_class2, 2: weight_class3}\n",
    "\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor = 'val_loss', \n",
    "    factor = 0.2, \n",
    "    verbose = 1, \n",
    "    patience = 5,                        \n",
    "    min_lr = 0.001\n",
    ")\n",
    "\n",
    "epoch_count=20\n",
    "batch_size= 128\n",
    "\n",
    "#running_model\n",
    "history = bilstm_model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs = epoch_count,\n",
    "    batch_size = batch_size,\n",
    "    validation_data = (x_val, y_val),\n",
    "    verbose = 1,\n",
    "    callbacks = [reduce_lr],\n",
    "    class_weight=class_weight\n",
    ")\n",
    "\n",
    "#plotting graphs\n",
    "def plot_learning_curves(history, arr):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "    for idx in range(2):\n",
    "        ax[idx].plot(history.history[arr[idx][0]])\n",
    "        ax[idx].plot(history.history[arr[idx][1]])\n",
    "        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=17)\n",
    "        ax[idx].set_xlabel('Loss ',fontsize=14)\n",
    "        ax[idx].set_ylabel('Accuracy',fontsize=14)\n",
    "        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)\n",
    "\n",
    "plot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])\n",
    "\n",
    "#prediciting\n",
    "preds= np.argmax(model.predict(X_test), axis=-1)\n",
    "#printing classification_report & confusion_matrix\n",
    "print(classification_report(y_test,preds ))\n",
    "print(confusion_matrix(y_test, preds))\n",
    "\n",
    "\n",
    "filters= 32\n",
    "kernel_size=2\n",
    "hidden_dims= 128\n",
    "    \n",
    "def CNN():\n",
    "    model = Sequential()\n",
    "    #adding embedding layer\n",
    "    model.add(Embedding(\n",
    "        input_dim=embedding_matrix.shape[0], \n",
    "        output_dim=embedding_matrix.shape[1], \n",
    "        weights = [embedding_matrix], \n",
    "        input_length=length_long_sentence))\n",
    "    # 2 CNN layer\n",
    "    model.add(Conv1D(32,2,padding='valid', activation='relu')) #cnn_layer_1\n",
    "    model.add(Conv1D(64,2,padding='valid',activation='relu')) #cnn_layer_2\n",
    "    model.add(GlobalMaxPooling1D()) #globalmaxpooling_layer\n",
    "    model.add(Dense(256, activation='relu')) #dense_layer\n",
    "    model.add(Dropout(0.1)) #dropout_layer\n",
    "    model.add(Dense(3, activation = 'softmax')) #classification layer\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "#builiding CNN model\n",
    "model2=CNN()\n",
    " \n",
    "#running mode\n",
    "history2 = model2.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs = epoch_count,\n",
    "    batch_size = batch_size,\n",
    "    validation_data = (x_val, y_val),\n",
    "    verbose = 1,\n",
    "    callbacks = [reduce_lr, checkpoint],\n",
    "    class_weight=class_weight\n",
    ")\n",
    "\n",
    "#plotting graphs\n",
    "plot_learning_curves(history2, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])\n",
    "#predicting\n",
    "pred2= np.argmax(model2.predict(X_test), axis=-1)\n",
    "#printing reports\n",
    "print(classification_report(y_test,pred2 ))\n",
    "print(confusion_matrix(y_test, pred2))\n",
    "\n",
    "def MLP():\n",
    "    model = Sequential()\n",
    "    #embedding layer\n",
    "    model.add(Embedding(\n",
    "        input_dim=embedding_matrix.shape[0], \n",
    "        output_dim=embedding_matrix.shape[1], \n",
    "        weights = [embedding_matrix], \n",
    "        input_length=length_long_sentence))\n",
    "    model.add(Flatten()) #flatten_layer\n",
    "    model.add(Dense(512, activation='relu')) #dense_layer\n",
    "    model.add(Dropout(0.2)) #dropout_layer\n",
    "    model.add(Dense(3, activation = 'softmax'))#classification_layer\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "#building model\n",
    "model3=MLP()\n",
    "#running_model\n",
    "history3 = model3.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs = epoch_count,\n",
    "    batch_size = batch_size,\n",
    "    validation_data = (x_val, y_val),\n",
    "    verbose = 1,\n",
    "    callbacks = [reduce_lr, checkpoint],\n",
    "    class_weight=class_weight\n",
    ")\n",
    "\n",
    "#plotting_graphs\n",
    "plot_learning_curves(history3, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])\n",
    "#predicting\n",
    "pred3= np.argmax(model3.predict(X_test), axis=-1)\n",
    "\n",
    "print(classification_report(y_test,pred3))\n",
    "print(confusion_matrix(y_test, pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded783f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
